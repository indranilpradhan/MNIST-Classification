{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Multilayer Perceptron(MLP), Convolutional Neural Network(CNN) as well as Support Vector Machines(SVM) to classify digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTiTjsxcsjEo"
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ZDpggZaTWiO"
   },
   "source": [
    "# Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoxagrwSx7Fz"
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('/content/data/')\n",
    "xtrain, ytrain = mndata.load_training()\n",
    "xtest, ytest = mndata.load_testing()\n",
    "xtrain = np.array(xtrain)\n",
    "ytrain = np.array(ytrain)\n",
    "xtest = np.array(xtest)\n",
    "ytest = np.array(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6Cjo3QN31xy"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the data between 0-254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70AMjbPKe0oO"
   },
   "outputs": [],
   "source": [
    "svmxtrain = xtrain/255.0\n",
    "svmytrain = ytrain\n",
    "svmxtest = xtest/255.0\n",
    "svmytest = ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying standard scaler to transform the data such that its distribution will have a mean value 0 and standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uYTnRHbcgTqA"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "svmxtrain = scaler.fit_transform(svmxtrain)\n",
    "svmxtest = scaler.transform(svmxtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1:- Applying Support Vector Machine with Linear kernel and calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7y5vEnvgXaAa",
    "outputId": "dffdee14-f081-4453-b34c-d455c98cc17c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9293\n"
     ]
    }
   ],
   "source": [
    "svc1 = svm.SVC(kernel='linear')\n",
    "svc1.fit(svmxtrain, svmytrain) \n",
    "y_predict1 = svc1.predict(svmxtest)\n",
    "acc1 = accuracy_score(svmytest, y_predict1)\n",
    "print('Accuracy = ',acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "0t8xXPFslF46",
    "outputId": "95510ea1-17e4-4d51-f107-a4706787ad84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 951    0    5    2    2    8    8    2    1    1]\n",
      " [   0 1119    6    2    0    1    2    1    4    0]\n",
      " [  10   13  956   11    7    4    5    6   18    2]\n",
      " [   7    1   15  941    0   16    1    6   19    4]\n",
      " [   3    2   18    1  929    0    3    5    4   17]\n",
      " [   7    6    7   41    6  789   12    2   19    3]\n",
      " [  12    3   13    1    8   17  902    0    2    0]\n",
      " [   2    8   23   13   10    1    0  945    5   21]\n",
      " [  12    6   11   28    8   24    9    6  858   12]\n",
      " [   6    7    6   10   36    4    1   23   13  903]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(svmytest, y_predict1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "15jgu7rVlReb",
    "outputId": "1d53e974-db6b-4b4d-cd7a-b15d58e05b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       980\n",
      "           1       0.96      0.99      0.97      1135\n",
      "           2       0.90      0.93      0.91      1032\n",
      "           3       0.90      0.93      0.91      1010\n",
      "           4       0.92      0.95      0.93       982\n",
      "           5       0.91      0.88      0.90       892\n",
      "           6       0.96      0.94      0.95       958\n",
      "           7       0.95      0.92      0.93      1028\n",
      "           8       0.91      0.88      0.90       974\n",
      "           9       0.94      0.89      0.92      1009\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(svmytest, y_predict1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2:- Applying Support Vector Machine with polynomial kernel and calculating the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XqPaRIYoYa2X",
    "outputId": "fefb1821-b1ba-4639-a329-3ce2d6bad650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9611\n"
     ]
    }
   ],
   "source": [
    "svc2 = svm.SVC(kernel='poly')\n",
    "svc2.fit(svmxtrain, svmytrain) \n",
    "y_predict2 = svc2.predict(svmxtest)\n",
    "acc2 = accuracy_score(svmytest, y_predict2)\n",
    "print('Accuracy = ',acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "hC0B45Bdltrt",
    "outputId": "9146ef19-6811-4e4a-badc-871586cf5a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 962    0    0    1    0    3    4    0   10    0]\n",
      " [   0 1122    3    0    2    1    4    0    3    0]\n",
      " [   6    0  973    3    3    1    2    7   37    0]\n",
      " [   0    0    1  971    1    3    2    5   22    5]\n",
      " [   0    0    2    0  955    0    6    2    4   13]\n",
      " [   2    1    3    3    2  863    6    1    9    2]\n",
      " [   5    3    1    0    8    8  919    0   14    0]\n",
      " [   1    8   11    2   12    0    0  958   11   25]\n",
      " [   1    0    3    6    5    8    1    3  943    4]\n",
      " [   3    4    1   11   24    3    0    4   14  945]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(svmytest, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "fTD1AFMElwLm",
    "outputId": "0f74876f-2a84-41ef-c1d8-bf48ffc33058"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.94      0.96      1032\n",
      "           3       0.97      0.96      0.97      1010\n",
      "           4       0.94      0.97      0.96       982\n",
      "           5       0.97      0.97      0.97       892\n",
      "           6       0.97      0.96      0.97       958\n",
      "           7       0.98      0.93      0.95      1028\n",
      "           8       0.88      0.97      0.92       974\n",
      "           9       0.95      0.94      0.94      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(svmytest, y_predict2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3:- Applying Support Vector Machine with RBF kernel and calculating the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kXBZh8k5i6pk",
    "outputId": "1bc77b1a-40a3-441d-e0d5-4e0ba103be66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.9661\n"
     ]
    }
   ],
   "source": [
    "svc3 = svm.SVC(kernel='rbf')\n",
    "svc3.fit(svmxtrain, svmytrain) \n",
    "y_predict3 = svc3.predict(svmxtest)\n",
    "acc3 = accuracy_score(svmytest, y_predict3)\n",
    "print('Accuracy = ',acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "DebLmvpTl73x",
    "outputId": "45908606-6549-4432-8c37-f1bb83dd55d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 968    0    1    1    0    3    3    2    2    0]\n",
      " [   0 1127    3    0    0    1    2    0    2    0]\n",
      " [   5    1  996    2    2    0    1   15    9    1]\n",
      " [   0    0    4  980    1    7    0   11    7    0]\n",
      " [   0    0   12    0  944    2    4    7    3   10]\n",
      " [   2    0    1   10    2  854    6    8    7    2]\n",
      " [   6    2    1    0    4    8  930    2    5    0]\n",
      " [   1    6   13    2    3    0    0  990    0   13]\n",
      " [   3    0    4    6    6    9    3   14  926    3]\n",
      " [   4    6    5   11   12    2    0   20    3  946]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(svmytest, y_predict3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "O2paHWaxl7qW",
    "outputId": "fde8eec1-321c-4872-cbae-e21e20598bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.96      0.97      0.96      1032\n",
      "           3       0.97      0.97      0.97      1010\n",
      "           4       0.97      0.96      0.97       982\n",
      "           5       0.96      0.96      0.96       892\n",
      "           6       0.98      0.97      0.98       958\n",
      "           7       0.93      0.96      0.94      1028\n",
      "           8       0.96      0.95      0.96       974\n",
      "           9       0.97      0.94      0.95      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(svmytest, y_predict3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-Z5RyF_jD5E"
   },
   "source": [
    "# Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bv1Lqo7unUbe"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training dataset and test dataset and reshaping it to the length of the dataset * 28 * 28 * 1 to feed into the multi layer perceptron and normalizing it between 0-254 and creating batches of size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4w8uAHyJkKj"
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('/content/data/')\n",
    "xtrain, ytrain = mndata.load_training()\n",
    "xtest, ytest = mndata.load_testing()\n",
    "original = np.array(ytest)\n",
    "xtrain = np.array(xtrain)\n",
    "ytrain = np.array(ytrain)\n",
    "xtest = np.array(xtest)\n",
    "ytest = np.array(ytest)\n",
    "xtrain = xtrain.reshape(xtrain.shape[0], 28, 28, 1)\n",
    "xtest = xtest.reshape(xtest.shape[0], 28, 28, 1)\n",
    "xtrain = xtrain.astype('float')\n",
    "xtest = xtest.astype('float')\n",
    "xtrain = xtrain/255\n",
    "xtest = xtest/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKrUf9mKmmQr"
   },
   "outputs": [],
   "source": [
    "xtrain = Tensor(xtrain)\n",
    "ytrain = Tensor(ytrain)\n",
    "ytrain = ytrain.long()\n",
    "xtest = Tensor(xtest)\n",
    "ytest = Tensor(ytest)\n",
    "trainset = TensorDataset(xtrain,ytrain)\n",
    "testset = TensorDataset(xtest, ytest)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2dwuHNVPJH2"
   },
   "source": [
    "# Experiment 4:- Multi layer perceptron of input layer consisting of 784(28 * 28) neurons, two hidden layers with 128 neurons and 64 neurons in each layer correspondingly. Output layers of 10 neurons for 10 classes. Applying ReLu(y = max(0, x)) activation function in both of the hidden layers. Linear activation function (A = cx) is used in output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NttjLtkun8hb"
   },
   "outputs": [],
   "source": [
    "inputlayer = 784\n",
    "hiddenlayer = [128, 64]\n",
    "outputlayer = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(inputlayer, hiddenlayer[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[0], hiddenlayer[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[1], outputlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss function is Cross Entropy loss (H(P, Q) = – sum x in X P(x) * log(Q(x)) ) and Stochastic Gradient descent optimzer is used with learning rate 0.003 and momentum 0.9. Epochs is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_88ofw0ssguE"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xtrain, ytrain in trainloader:\n",
    "        xtrain = xtrain.view(xtrain.shape[0], -1)        \n",
    "        optimizer.zero_grad()        \n",
    "        result = model(xtrain)\n",
    "        loss = criterion(result, ytrain)       \n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the test data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MHLExxqzsuuJ",
    "outputId": "82a470dc-bd0b-4db6-b1b6-f3951338ec80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy = 0.9793\n"
     ]
    }
   ],
   "source": [
    "correct_prediction, total_count = 0, 0\n",
    "original1 = []\n",
    "predictions1 =[]\n",
    "for xtest,ytest in testloader:\n",
    "    for i in range(len(ytest)):\n",
    "        img = xtest[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            ps = model(img)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        prediction = probab.index(max(probab))\n",
    "        real = ytest.numpy()[i]\n",
    "        original1.append(real)\n",
    "        predictions1.append(prediction)\n",
    "        if(real == prediction):\n",
    "            correct_prediction += 1\n",
    "        total_count += 1\n",
    "print(\"\\nModel Accuracy =\", (correct_prediction/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "DZrIGeIsZVu7",
    "outputId": "0b9cb8f4-10db-45a6-bcfe-2ccfd994417d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    1    1    1    1    0    3    1    2    1]\n",
      " [   0 1125    3    1    0    1    2    1    2    0]\n",
      " [   4    2 1007    6    3    0    1    4    5    0]\n",
      " [   0    0    3  992    0    3    0    4    4    4]\n",
      " [   1    0    2    1  960    0    4    3    1   10]\n",
      " [   3    0    0   10    2  866    4    1    3    3]\n",
      " [   3    2    2    1    4    3  941    0    2    0]\n",
      " [   0    3    7    1    0    0    0 1011    1    5]\n",
      " [   5    0    3    8    3    3    3    4  940    5]\n",
      " [   2    2    0    4    7    4    1    3    4  982]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original1, predictions1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "LciSsHd9ZZlZ",
    "outputId": "c134c2ad-c522-48b5-effb-f8ac5259c1ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.98      0.98      0.98      1032\n",
      "         3.0       0.97      0.98      0.97      1010\n",
      "         4.0       0.98      0.98      0.98       982\n",
      "         5.0       0.98      0.97      0.98       892\n",
      "         6.0       0.98      0.98      0.98       958\n",
      "         7.0       0.98      0.98      0.98      1028\n",
      "         8.0       0.98      0.97      0.97       974\n",
      "         9.0       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original1, predictions1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-yTUNHhPO6C"
   },
   "source": [
    "# Experiment 5:- Multi layer perceptron of input layer consisting of 784(28 * 28) neurons, three hidden layers with 128 neurons and 64 neurons and 32 in each layer correspondingly. Output layers of 10 neurons for 10 classes. Applying ReLu(y = max(0, x)) activation function in all of the hidden layers. Linear activation function (A = cx) is used in output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EoCMFbTUPTp4"
   },
   "outputs": [],
   "source": [
    "inputlayer = 784\n",
    "hiddenlayer = [128, 64, 32]\n",
    "outputlayer = 10\n",
    "\n",
    "model2 = nn.Sequential(nn.Linear(inputlayer, hiddenlayer[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[0], hiddenlayer[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[1], hiddenlayer[2]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[2], outputlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss function is Cross Entropy loss (H(P, Q) = – sum x in X P(x) * log(Q(x)) ) and Stochastic Gradient descent optimzer is used with learning rate 0.003 and momentum 0.9. Epochs is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uvxf_rHoPTiA"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.003, momentum=0.9)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xtrain, ytrain in trainloader:\n",
    "        xtrain = xtrain.view(xtrain.shape[0], -1)        \n",
    "        optimizer.zero_grad()        \n",
    "        result = model2(xtrain)\n",
    "        loss = criterion(result, ytrain)       \n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the test data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ybaAoGvyPTbH",
    "outputId": "05945458-61fc-4721-b12b-ed2512573977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy = 0.9775\n"
     ]
    }
   ],
   "source": [
    "correct_prediction, total_count = 0, 0\n",
    "original2 = []\n",
    "predictions2 = []\n",
    "for xtest,ytest in testloader:\n",
    "    for i in range(len(ytest)):\n",
    "        img = xtest[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            ps = model2(img)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        prediction = probab.index(max(probab))\n",
    "        real = ytest.numpy()[i]\n",
    "        original2.append(real)\n",
    "        predictions2.append(prediction)\n",
    "        if(real == prediction):\n",
    "            correct_prediction += 1\n",
    "        total_count += 1\n",
    "print(\"\\nModel Accuracy =\", (correct_prediction/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "l9tF0hX5m3s0",
    "outputId": "e3eb26ac-752d-4df6-bb77-aa667c264cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 971    0    1    0    0    1    3    1    2    1]\n",
      " [   0 1128    2    0    0    0    2    1    2    0]\n",
      " [   6    3 1002    5    2    0    3    5    6    0]\n",
      " [   0    0    3  987    0    4    0    7    5    4]\n",
      " [   1    0    2    1  960    0    4    3    0   11]\n",
      " [   2    0    0   11    1  865    4    2    4    3]\n",
      " [   4    3    5    1    5    6  933    0    1    0]\n",
      " [   2    7    4    5    1    0    1  999    3    6]\n",
      " [   5    1    3    5    3    3    1    2  947    4]\n",
      " [   3    3    0    3    9    4    0    2    2  983]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original2, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "SKl-swxMm9jm",
    "outputId": "da959432-4226-4ba6-e642-0ed73385f104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.98      0.97      0.98      1032\n",
      "         3.0       0.97      0.98      0.97      1010\n",
      "         4.0       0.98      0.98      0.98       982\n",
      "         5.0       0.98      0.97      0.97       892\n",
      "         6.0       0.98      0.97      0.98       958\n",
      "         7.0       0.98      0.97      0.97      1028\n",
      "         8.0       0.97      0.97      0.97       974\n",
      "         9.0       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original2, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tgx7B9yVPp6k"
   },
   "source": [
    "# Experiment 6:- Multi layer perceptron of input layer consisting of 784(28 * 28) neurons, three hidden layers with 128 neurons and 64 neurons and 32 in each layer correspondingly. Output layers of 10 neurons for 10 classes. Applying sigmoid(y = 1/(1+e^(-x)) activation function in all of the hidden layers. Linear activation function (A = cx) is used in output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ut86Y7ZPsfG"
   },
   "outputs": [],
   "source": [
    "inputlayer = 784\n",
    "hiddenlayer = [128, 64, 32]\n",
    "outputlayer = 10\n",
    "\n",
    "model3 = nn.Sequential(nn.Linear(inputlayer, hiddenlayer[0]),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hiddenlayer[0], hiddenlayer[1]),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hiddenlayer[1], hiddenlayer[2]),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hiddenlayer[2], outputlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss function is Cross Entropy loss (H(P, Q) = – sum x in X P(x) * log(Q(x)) ) and Stochastic Gradient descent optimzer is used with learning rate 0.003 and momentum 0.9. Epochs is 100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jgO4m9VPsYI"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model3.parameters(), lr=0.003, momentum=0.9)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xtrain, ytrain in trainloader:\n",
    "        xtrain = xtrain.view(xtrain.shape[0], -1)        \n",
    "        optimizer.zero_grad()        \n",
    "        result = model3(xtrain)\n",
    "        loss = criterion(result, ytrain)       \n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the test data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Uc2VmP13PsR5",
    "outputId": "0c9a7c0f-bfa2-44ff-b5ec-33f8ba5202e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy = 0.9673\n"
     ]
    }
   ],
   "source": [
    "correct_prediction, total_count = 0, 0\n",
    "original3 = []\n",
    "predictions3 = []\n",
    "for xtest,ytest in testloader:\n",
    "    for i in range(len(ytest)):\n",
    "        img = xtest[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            ps = model3(img)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        prediction = probab.index(max(probab))\n",
    "        real = ytest.numpy()[i]\n",
    "        original3.append(real)\n",
    "        predictions3.append(prediction)\n",
    "        if(real == prediction):\n",
    "            correct_prediction += 1\n",
    "        total_count += 1\n",
    "print(\"\\nModel Accuracy =\", (correct_prediction/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "8QS3pfMsnIhh",
    "outputId": "84576561-a76a-465e-df7d-52f68d3a9c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 966    0    4    0    1    5    1    1    2    0]\n",
      " [   0 1121    2    5    0    1    0    2    4    0]\n",
      " [   4    5  999    8    1    0    2    9    4    0]\n",
      " [   0    1    3  986    0    7    0    7    5    1]\n",
      " [   4    0    2    0  943    0    6    6    1   20]\n",
      " [   5    0    1   25    1  846    1    0   10    3]\n",
      " [  11    2    6    0    8    5  922    0    4    0]\n",
      " [   1   10    6    6    1    0    0  996    0    8]\n",
      " [   1    3    4    8    4   11    3    4  932    4]\n",
      " [   3    3    0    6    8    9    1   16    1  962]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original3, predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "kwot4KmdnJcy",
    "outputId": "d52a2670-ffc7-4db6-e55b-ed8e7249c991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       980\n",
      "         1.0       0.98      0.99      0.98      1135\n",
      "         2.0       0.97      0.97      0.97      1032\n",
      "         3.0       0.94      0.98      0.96      1010\n",
      "         4.0       0.98      0.96      0.97       982\n",
      "         5.0       0.96      0.95      0.95       892\n",
      "         6.0       0.99      0.96      0.97       958\n",
      "         7.0       0.96      0.97      0.96      1028\n",
      "         8.0       0.97      0.96      0.96       974\n",
      "         9.0       0.96      0.95      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original3, predictions3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDhpdpReUff9"
   },
   "source": [
    "# Experiment 7:- Multi layer perceptron of input layer consisting of 784(28 * 28) neurons, three hidden layers with 128 neurons and 64 neurons and 32 in each layer correspondingly. Output layers of 10 neurons for 10 classes. Applying ReLu(y = max(a,0)) in the first two hidden layers. Applying sigmoid(y = 1/(1+e^(-x)) activation function in the third hidden layer. Linear activation function (A = cx) is used in output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KZH1N3pT7TB"
   },
   "outputs": [],
   "source": [
    "inputlayer = 784\n",
    "hiddenlayer = [128, 64,32]\n",
    "outputlayer = 10\n",
    "\n",
    "model5 = nn.Sequential(nn.Linear(inputlayer, hiddenlayer[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[0], hiddenlayer[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hiddenlayer[1], hiddenlayer[2]),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hiddenlayer[2], outputlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss function is Cross Entropy loss (H(P, Q) = – sum x in X P(x) * log(Q(x)) ) and Stochastic Gradient descent optimzer is used with learning rate 0.003 and momentum 0.9. Epoch is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74V4PUXST7Mz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model5.parameters(), lr=0.003, momentum=0.9)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xtrain, ytrain in trainloader:\n",
    "        xtrain = xtrain.view(xtrain.shape[0], -1)        \n",
    "        optimizer.zero_grad()        \n",
    "        result = model5(xtrain)\n",
    "        loss = criterion(result, ytrain)       \n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the test data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4LezBBkrT7EJ",
    "outputId": "256017a1-4889-4120-c076-04b3ac23f537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy = 0.977\n"
     ]
    }
   ],
   "source": [
    "correct_prediction, total_count = 0, 0\n",
    "predictions5 = []\n",
    "original5 = []\n",
    "for xtest,ytest in testloader:\n",
    "    for i in range(len(ytest)):\n",
    "        img = xtest[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            ps = model5(img)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        prediction = probab.index(max(probab))\n",
    "        real = ytest.numpy()[i]\n",
    "        original5.append(real)\n",
    "        predictions5.append(prediction)\n",
    "        if(real == prediction):\n",
    "            correct_prediction += 1\n",
    "        total_count += 1\n",
    "print(\"\\nModel Accuracy =\", (correct_prediction/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "CqO6N4HFT6_V",
    "outputId": "a3f44b92-7c47-40e1-b345-15768aa545f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    0    2    1    1    2    2    1    1    1]\n",
      " [   0 1125    3    0    0    1    3    1    2    0]\n",
      " [   2    1 1010    7    1    0    3    4    4    0]\n",
      " [   0    0    4  986    0   10    0    4    4    2]\n",
      " [   0    1    4    0  955    0    6    2    2   12]\n",
      " [   4    0    0   10    1  861    6    1    4    5]\n",
      " [   6    3    0    1    5    4  937    0    2    0]\n",
      " [   1    6    5    3    2    0    0 1000    2    9]\n",
      " [   2    0    3    6    6    6    3    1  942    5]\n",
      " [   4    2    0    2    7    3    0    3    3  985]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original5,predictions5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "OKHrMI8ET634",
    "outputId": "aa2bf39a-7106-418a-b17e-8282d1bc5c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.98      0.98      0.98      1032\n",
      "         3.0       0.97      0.98      0.97      1010\n",
      "         4.0       0.98      0.97      0.97       982\n",
      "         5.0       0.97      0.97      0.97       892\n",
      "         6.0       0.98      0.98      0.98       958\n",
      "         7.0       0.98      0.97      0.98      1028\n",
      "         8.0       0.98      0.97      0.97       974\n",
      "         9.0       0.97      0.98      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original5, predictions5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BkpBcp2VBq6r"
   },
   "source": [
    "# Experiment 7:- Multi layer perceptron of input layer consisting of 784(28 * 28) neurons, three hidden layers with 128 neurons and 64 neurons and 32 in each layer correspondingly. Output layers of 10 neurons for 10 classes. Applying Tanh(f(x) = 1 — exp(-2x) / 1 + exp(-2x)) in the first two hidden layers. Applying sigmoid(y = 1/(1+e^(-x)) activation function in the third hidden layer. Linear activation function (A = cx) is used in output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVLbczucBwZJ"
   },
   "outputs": [],
   "source": [
    "inputlayer = 784\n",
    "hiddenlayer = [128, 64,32]\n",
    "outputlayer = 10\n",
    "\n",
    "model6 = nn.Sequential(nn.Linear(inputlayer, hiddenlayer[0]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hiddenlayer[0], hiddenlayer[1]),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(hiddenlayer[1], hiddenlayer[2]),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(hiddenlayer[2], outputlayer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The loss function is Cross Entropy loss (H(P, Q) = – sum x in X P(x) * log(Q(x)) ) and Stochastic Gradient descent optimzer is used with learning rate 0.003 and momentum 0.9. Epoch is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8F8QB0L7Bxh_"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model6.parameters(), lr=0.003, momentum=0.9)\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xtrain, ytrain in trainloader:\n",
    "        xtrain = xtrain.view(xtrain.shape[0], -1)        \n",
    "        optimizer.zero_grad()        \n",
    "        result = model6(xtrain)\n",
    "        loss = criterion(result, ytrain)       \n",
    "        loss.backward()\n",
    "        optimizer.step()       \n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the test data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "e6x8-_w9BxWG",
    "outputId": "39a8436b-2033-4a8d-b537-5a656246a2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy = 0.9772\n"
     ]
    }
   ],
   "source": [
    "correct_prediction, total_count = 0, 0\n",
    "predictions6 = []\n",
    "original6 = []\n",
    "for xtest,ytest in testloader:\n",
    "    for i in range(len(ytest)):\n",
    "        img = xtest[i].view(1, 784)\n",
    "        with torch.no_grad():\n",
    "            ps = model6(img)\n",
    "        probab = list(ps.numpy()[0])\n",
    "        prediction = probab.index(max(probab))\n",
    "        real = ytest.numpy()[i]\n",
    "        original6.append(real)\n",
    "        predictions6.append(prediction)\n",
    "        if(real == prediction):\n",
    "            correct_prediction += 1\n",
    "        total_count += 1\n",
    "print(\"\\nModel Accuracy =\", (correct_prediction/total_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "SlTgO-RjBwQ-",
    "outputId": "a5edae12-0f8d-4a58-ae67-caa6ee766fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 966    0    2    0    1    3    2    1    3    2]\n",
      " [   0 1127    2    1    0    0    1    1    3    0]\n",
      " [   3    3 1010    5    1    0    1    3    6    0]\n",
      " [   0    0    4  986    0    5    0    3   10    2]\n",
      " [   1    1    1    0  960    1    6    1    1   10]\n",
      " [   4    0    0    9    1  863    2    3    6    4]\n",
      " [   5    2    3    0    3    4  939    0    2    0]\n",
      " [   0    4    9    3    1    1    0  998    1   11]\n",
      " [   4    1    4    4    2    6    2    2  946    3]\n",
      " [   0    2    0    4   10    2    1    9    4  977]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original6,predictions6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "RfJINw8TBwJF",
    "outputId": "33fe469b-502b-44a1-a687-f04c1d21f394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.98      0.98      0.98      1032\n",
      "         3.0       0.97      0.98      0.98      1010\n",
      "         4.0       0.98      0.98      0.98       982\n",
      "         5.0       0.98      0.97      0.97       892\n",
      "         6.0       0.98      0.98      0.98       958\n",
      "         7.0       0.98      0.97      0.97      1028\n",
      "         8.0       0.96      0.97      0.97       974\n",
      "         9.0       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original6, predictions6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Nh03SekfQDj"
   },
   "source": [
    "# Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gWL-rR8kfhxX",
    "outputId": "73100c58-d072-4d23-fcd5-771402ce2750"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from torchvision import datasets, transforms\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "# from torch import optim\n",
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import keras\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the training dataset and test dataset and reshaping it to the length of the dataset * 28 * 28 * 1 to feed into the multi layer perceptron and normalizing it between 0-254."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xzDIt4bofiqg"
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('G:\\second_sem\\SMAI\\Assignment_3\\q3\\dataset')\n",
    "xtrain, ytrain = mndata.load_training()\n",
    "xtest, ytest = mndata.load_testing()\n",
    "xtrain = np.array(xtrain)\n",
    "ytrain = keras.utils.to_categorical(ytrain, 10)\n",
    "xtest = np.array(xtest)\n",
    "original = np.array(ytest)\n",
    "ytest = keras.utils.to_categorical(ytest, 10)\n",
    "xtrain = xtrain.reshape(xtrain.shape[0], 28, 28, 1)\n",
    "xtest = xtest.reshape(xtest.shape[0], 28, 28, 1)\n",
    "xtrain = xtrain.astype('float')\n",
    "xtest = xtest.astype('float')\n",
    "xtrain = xtrain/255\n",
    "xtest = xtest/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4AzoTgUAii4"
   },
   "source": [
    "# Experiment 8:- The model is comprised of one output layer and one 2d convolution layer and one hidden layer. On the hidden layer and 2d convolution layer ReLu activation function has been used. Softmax activation function is used in output layer. 2D convolution layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. The number of output filters in the convolution is 32. Kernel of size 3x3 is used. After that max pooling of 2x2 matrix is used. The dropout rate is set to 30% from convolution layer to the hidden layer, meaning one in  almost 3 inputs will be randomly excluded from each update cycle. The convoltuion layer to hidden layer is densed and 128 neurons is used in the hidden layer. From the hidden layer to output layer dropout rate is set 25% meaning one in 4 inputs will be discarded randomly and it is densed. 10 neuronse for 10 classes is used in output layer. Stochastic gradient descent optimizer is used with learning rate 0.003 and momentum 0.9. The loss funtion is used cross entropy loss. And the metrics for the model is \"accuracy\". Batch size of 64 is created with epochs is set 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "ACOEdCEKvjWg",
    "outputId": "423770de-a7fc-48b1-de24-440bd5ac95a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 33s 558us/step - loss: 0.5601 - acc: 0.8288\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 33s 551us/step - loss: 0.2524 - acc: 0.9228\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 33s 551us/step - loss: 0.1992 - acc: 0.9394\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 33s 553us/step - loss: 0.1717 - acc: 0.9481\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 33s 547us/step - loss: 0.1505 - acc: 0.9541\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 33s 548us/step - loss: 0.1393 - acc: 0.9569\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 34s 567us/step - loss: 0.1261 - acc: 0.9617\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 33s 551us/step - loss: 0.1156 - acc: 0.9644\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 33s 549us/step - loss: 0.1059 - acc: 0.9673\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 33s 548us/step - loss: 0.0999 - acc: 0.9696\n",
      "10000/10000 [==============================] - 2s 228us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0648645735614933, 0.9797]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',input_shape=(28,28,1)))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.30))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(units=128, activation='relu'))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Dense(units=10, activation='softmax'))\n",
    "optimizer = SGD(lr=0.003, momentum=0.9)\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer, metrics=['accuracy'])\n",
    "model1.fit(x=xtrain,y=ytrain,batch_size=64,epochs=10)\n",
    "acc1 = model1.evaluate(xtest, ytest)\n",
    "acc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDkq2Gmv8xas"
   },
   "outputs": [],
   "source": [
    "temp1 = model1.predict(xtest)\n",
    "pred1 = np.argmax(np.round(temp1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "9x8yx_nj-Mj6",
    "outputId": "a091f55c-b3d5-4c3c-99e5-8e7cb01a4b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 976    0    0    0    0    0    0    1    3    0]\n",
      " [   2 1124    2    1    0    0    2    0    4    0]\n",
      " [  13    0 1007    1    1    0    0    4    6    0]\n",
      " [   3    0    1  994    0    1    0    4    7    0]\n",
      " [   7    0    6    0  941    0    1    1    2   24]\n",
      " [  10    0    0   10    0  863    2    1    5    1]\n",
      " [  11    3    1    1    4    4  931    0    3    0]\n",
      " [  14    7   10    1    0    0    0  984    3    9]\n",
      " [   9    0    2    5    0    2    0    2  952    2]\n",
      " [  11    5    0    7    6    2    0    4    2  972]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original,pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "8X1ukB39-47C",
    "outputId": "0209deb8-ff4a-48a3-e022-2c7f76c46cae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.98      0.98      1032\n",
      "           3       0.97      0.98      0.98      1010\n",
      "           4       0.99      0.96      0.97       982\n",
      "           5       0.99      0.97      0.98       892\n",
      "           6       0.99      0.97      0.98       958\n",
      "           7       0.98      0.96      0.97      1028\n",
      "           8       0.96      0.98      0.97       974\n",
      "           9       0.96      0.96      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original,pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I33Q2MaaAl2d"
   },
   "source": [
    "# Experiment 9:- The model is comprised of one output layer and one 2d convolution layer and one hidden layer. On the hidden layer and 2d convolution layer ReLu activation function has been used. Softmax activation function is used in output layer. 2D convolution layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. The number of output filters in the convolution is 64. Kernel of size 4x4 is used. After that max pooling of 2x2 matrix is used. The dropout rate is set to 50% from convolution layer to the hidden layer, meaning one in 2 inputs will be randomly excluded from each update cycle. The convoltuion layer to hidden layer is densed and 128 neurons is used in the hidden layer. From the hidden layer to output layer dropout rate is set 20% meaning one in 5 inputs will be discarded randoml and it is densed. 10 neuronse for 10 classes is used in output layer. Stochastic gradient descent optimizer is used with learning rate 0.003 and momentum 0.9. The loss funtion is used cross entropy loss. And the metrics for the model is \"accuracy\". Batch size of 64 is created with epochs is set 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "I6GWP_I-yHv4",
    "outputId": "d3658f01-a960-463d-87f3-c67c329bb540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 50s 840us/step - loss: 0.5143 - acc: 0.8440\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 51s 849us/step - loss: 0.2270 - acc: 0.9303\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 50s 831us/step - loss: 0.1683 - acc: 0.9492\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 50s 827us/step - loss: 0.1356 - acc: 0.9594\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 50s 828us/step - loss: 0.1162 - acc: 0.9651\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 50s 833us/step - loss: 0.1025 - acc: 0.9696\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 50s 828us/step - loss: 0.0926 - acc: 0.9719\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 50s 827us/step - loss: 0.0838 - acc: 0.9740\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 51s 846us/step - loss: 0.0760 - acc: 0.9769\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 51s 846us/step - loss: 0.0718 - acc: 0.9784\n",
      "10000/10000 [==============================] - 2s 241us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04387621255386621, 0.9851]"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(filters=64, kernel_size=(4, 4), activation='relu',input_shape=(28,28,1)))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units=128, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(units=10, activation='softmax'))\n",
    "optimizer = SGD(lr=0.003, momentum=0.9)\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer, metrics=['accuracy'])\n",
    "model2.fit(x=xtrain,y=ytrain,batch_size=64,epochs=10)\n",
    "acc2 = model2.evaluate(xtest, ytest)\n",
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ld92GLbY_eJD"
   },
   "outputs": [],
   "source": [
    "temp2 = model2.predict(xtest)\n",
    "pred2 = np.argmax(np.round(temp2),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WJWcmbEu_jYV",
    "outputId": "70bfed37-e0a4-4561-b595-f152d449edec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 977    0    0    0    0    0    0    1    2    0]\n",
      " [   3 1126    2    1    0    0    1    0    2    0]\n",
      " [   7    1 1014    0    2    0    0    6    2    0]\n",
      " [   3    0    3  997    0    2    0    2    3    0]\n",
      " [   6    0    1    0  972    0    0    0    1    2]\n",
      " [   2    0    0    4    0  880    3    1    1    1]\n",
      " [  11    3    0    1    3    3  937    0    0    0]\n",
      " [   5    1    7    1    0    0    0 1010    1    3]\n",
      " [   7    1    2    3    1    2    1    3  950    4]\n",
      " [  13    5    0    1    9    0    0    4    1  976]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original,pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "GTbcIaxA_o0W",
    "outputId": "aa7dbed3-0bac-491e-95e0-f41f68c8726f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.99      0.98      0.98      1032\n",
      "           3       0.99      0.99      0.99      1010\n",
      "           4       0.98      0.99      0.99       982\n",
      "           5       0.99      0.99      0.99       892\n",
      "           6       0.99      0.98      0.99       958\n",
      "           7       0.98      0.98      0.98      1028\n",
      "           8       0.99      0.98      0.98       974\n",
      "           9       0.99      0.97      0.98      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original,pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uSvfqvgAokS"
   },
   "source": [
    "# Experiment 10:- The model is comprised of one output layer and one 2d convolution layer and one hidden layer. On the hidden layer and 2d convolution layer ReLu activation function has been used. Softmax activation function is used in output layer. 2D convolution layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. The number of output filters in the convolution is 32. Kernel of size 3x3 is used. After that max pooling of 2x2 matrix is used. The dropout rate is set to 20% from convolution layer to the hidden layer, meaning one in 5 inputs will be randomly excluded from each update cycle. The convoltuion layer to hidden layer is densed and 64 neurons is used in the hidden layer. From the hidden layer to output layer dropout rate is set 10% meaning one in 10 inputs will be discarded randoml and it is densed. 10 neuronse for 10 classes is used in output layer. Stochastic gradient descent optimizer is used with learning rate 0.003 and momentum 0.9. The loss funtion is used cross entropy loss. And the metrics for the model is \"accuracy\". Batch size of 64 is created with epochs is set 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "Kgs0LW_Ai43x",
    "outputId": "1aa3dd0b-42d1-4379-fb7f-1f0d41ae023c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 24s 404us/step - loss: 0.4256 - acc: 0.8799\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 24s 397us/step - loss: 0.1935 - acc: 0.9422\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 24s 397us/step - loss: 0.1447 - acc: 0.9569\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 24s 397us/step - loss: 0.1170 - acc: 0.9649\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 24s 393us/step - loss: 0.0977 - acc: 0.9703\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 23s 391us/step - loss: 0.0842 - acc: 0.9743\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 23s 390us/step - loss: 0.0745 - acc: 0.9776\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 24s 398us/step - loss: 0.0654 - acc: 0.9804\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 24s 399us/step - loss: 0.0593 - acc: 0.9822\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 24s 402us/step - loss: 0.0540 - acc: 0.9836\n",
      "10000/10000 [==============================] - 2s 208us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07566864336489235, 0.9758]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',input_shape=(28,28,1)))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(LeakyReLU(alpha=0.2))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(units=64, activation='relu'))\n",
    "model3.add(LeakyReLU(alpha=0.1))\n",
    "model3.add(Dense(units=10, activation='softmax'))\n",
    "optimizer = SGD(lr=0.003, momentum=0.9)\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer, metrics=['accuracy'])\n",
    "model3.fit(x=xtrain,y=ytrain,batch_size=64,epochs=10)\n",
    "acc3 = model3.evaluate(xtest, ytest)\n",
    "acc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zd1kjHLP_6KV"
   },
   "outputs": [],
   "source": [
    "temp3 = model3.predict(xtest)\n",
    "pred3 = np.argmax(np.round(temp3),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consfusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "bbcJXFAT_1pz",
    "outputId": "14996e2b-1a5c-4d3e-d5de-7e185d8824b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 974    0    1    0    0    0    1    1    3    0]\n",
      " [   2 1123    4    0    0    1    4    0    1    0]\n",
      " [   7    0 1009    1    2    0    1    3    9    0]\n",
      " [   7    0    9  980    0    0    0    3    7    4]\n",
      " [   2    0    4    0  973    0    1    0    1    1]\n",
      " [  12    0    0    9    1  855    4    1    9    1]\n",
      " [  14    2    0    1    3    2  933    0    3    0]\n",
      " [   5    6   14    1    0    0    0  996    3    3]\n",
      " [  10    0    1    2    3    0    1    1  956    0]\n",
      " [  21    6    1    7   16    2    0    9    7  940]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original,pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "aNjI9edp_x3r",
    "outputId": "568b9d5e-8b2b-4bd3-87e3-7adf1c1caf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.98      0.97      1032\n",
      "           3       0.98      0.97      0.97      1010\n",
      "           4       0.97      0.99      0.98       982\n",
      "           5       0.99      0.96      0.98       892\n",
      "           6       0.99      0.97      0.98       958\n",
      "           7       0.98      0.97      0.98      1028\n",
      "           8       0.96      0.98      0.97       974\n",
      "           9       0.99      0.93      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original,pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIuKR4EJHfco"
   },
   "source": [
    "# Experiment 11:- The model is comprised of one output layer and one 2d convolution layer and one hidden layer. On the hidden layer and 2d convolution layer Tanh activation function has been used. Softmax activation function is used in output layer. 2D convolution layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. The number of output filters in the convolution is 64. Kernel of size 2x2 is used. After that max pooling of 2x2 matrix is used. The dropout rate is set to 30% from convolution layer to the hidden layer, meaning one in almost 3 inputs will be randomly excluded from each update cycle. The convoltuion layer to hidden layer is densed and 128 neurons is used in the hidden layer. From the hidden layer to output layer dropout rate is set 20% meaning one in 5 inputs will be discarded randoml and it is densed. 10 neuronse for 10 classes is used in output layer. Stochastic gradient descent optimizer is used with learning rate 0.003 and momentum 0.9. The loss funtion is used cross entropy loss. And the metrics for the model is \"accuracy\". Batch size of 64 is created with epochs is set 10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "SV_lLLQjHRUc",
    "outputId": "659960dc-c6df-49c8-8440-01aa4f4bb446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.7211 - acc: 0.7884\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 18s 295us/step - loss: 0.4089 - acc: 0.8733\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 18s 296us/step - loss: 0.3310 - acc: 0.8982\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 18s 295us/step - loss: 0.2722 - acc: 0.9171\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.2326 - acc: 0.9295\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.2073 - acc: 0.9378\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 0.1897 - acc: 0.9422\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 18s 299us/step - loss: 0.1763 - acc: 0.9458\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.1664 - acc: 0.9489\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 18s 297us/step - loss: 0.1591 - acc: 0.9516\n",
      "10000/10000 [==============================] - 1s 106us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.115869736199826, 0.964]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Conv2D(filters=64, kernel_size=(2, 2), strides=(2, 2), activation='tanh',input_shape=(28,28,1)))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model4.add(Dropout(0.3))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(units=128, activation='tanh'))\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(units=10, activation='softmax'))\n",
    "optimizer = SGD(lr=0.003, momentum=0.9)\n",
    "model4.compile(loss=keras.losses.categorical_crossentropy,optimizer=optimizer, metrics=['accuracy'])\n",
    "model4.fit(x=xtrain,y=ytrain,batch_size=64,epochs=10)\n",
    "acc4 = model4.evaluate(xtest, ytest)\n",
    "acc4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKL2boGMMd0g"
   },
   "outputs": [],
   "source": [
    "temp4 = model4.predict(xtest)\n",
    "pred4 = np.argmax(np.round(temp4),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "cdzRnCFdHQzV",
    "outputId": "70151532-9573-4446-b074-6e8c80cf4af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 972    0    0    0    0    1    1    3    3    0]\n",
      " [   4 1122    3    0    0    1    3    1    1    0]\n",
      " [  24    2  981    5    4    0    3    5    7    1]\n",
      " [  10    0    3  980    0    5    1    5    4    2]\n",
      " [  11    1    2    0  935    0    8    4    3   18]\n",
      " [  26    1    1   15    0  836    8    2    3    0]\n",
      " [  13    2    1    0    2    8  931    0    1    0]\n",
      " [  25    4   15    2    1    0    0  965    1   15]\n",
      " [  23    0    6   10    4    2    2    3  920    4]\n",
      " [  28    2    1    7   10    4    0   13    1  943]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(original,pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "jKamAKg0HQqQ",
    "outputId": "ceec7d9e-a757-47f5-ccda-604e3682d775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.95      0.96      1032\n",
      "           3       0.96      0.97      0.97      1010\n",
      "           4       0.98      0.95      0.96       982\n",
      "           5       0.98      0.94      0.96       892\n",
      "           6       0.97      0.97      0.97       958\n",
      "           7       0.96      0.94      0.95      1028\n",
      "           8       0.97      0.94      0.96       974\n",
      "           9       0.96      0.93      0.95      1009\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(original,pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0TBFwNAzJnrX"
   },
   "source": [
    "# Summary : The best performing model is as The model is comprised of one output layer and one 2d convolution layer and one hidden layer. On the hidden layer and 2d convolution layer ReLu activation function has been used. Softmax activation function is used in output layer. 2D convolution layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. The number of output filters in the convolution is 64. Kernel of size 4x4 is used. After that max pooling of 2x2 matrix is used. The dropout rate is set to 50% from convolution layer to the hidden layer, meaning one in 2 inputs will be randomly excluded from each update cycle. The convoltuion layer to hidden layer is densed and 128 neurons is used in the hidden layer. From the hidden layer to output layer dropout rate is set 20% meaning one in 5 inputs will be discarded randoml and it is densed. 10 neuronse for 10 classes is used in output layer. Stochastic gradient descent optimizer is used with learning rate 0.003 and momentum 0.9. The loss funtion is used cross entropy loss. And the metrics for the model is \"accuracy\". Batch size of 64 is created with epochs is set 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "q3_a3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
